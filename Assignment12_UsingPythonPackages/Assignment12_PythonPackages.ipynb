{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 13: Using Python Packages\n",
    "Tierney O'Sullivan\n",
    "November 27, 2022\n",
    "\n",
    "### Library: nltk & spacy\n",
    "\n",
    "nltk [documentation](https://www.nltk.org/)  \n",
    "\n",
    "nltk, or \"Natural Language Tool kit\" is a library for working with text data.  \n",
    "Common uses include:\n",
    "\n",
    "- text classification\n",
    "- tokenization\n",
    "- stemming\n",
    "- tagging\n",
    "- parsing \n",
    "- semantic reasoning\n",
    "\n",
    "Here, we will use it's library as a tool to preprocess text data from Twitter which can then be used for further NLP analysis like clustering or sentiment analysis.  \n",
    "\n",
    "spacy [documentation](https://spacy.io/)  \n",
    "\n",
    "SpaCy is a newer NLP library and has some faster algorithms, since it has been implemented in Cython. \n",
    "\n",
    "## Twitter data\n",
    "The data set we are using today is from kaggle, and includes tweets related to COVID-19 vaccines and can be downloaded [here](https://www.kaggle.com/datasets/gpreda/all-covid19-vaccines-tweets).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "First we will import libraries needed for this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from IPython.display import display, HTML\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy #lemmatization\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('vaccination_all_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_created</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_friends</th>\n",
       "      <th>user_favourites</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>source</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1340539111971516416</td>\n",
       "      <td>Rachel Roh</td>\n",
       "      <td>La Crescenta-Montrose, CA</td>\n",
       "      <td>Aggregator of Asian American news; scanning di...</td>\n",
       "      <td>2009-04-08 17:52:46</td>\n",
       "      <td>405</td>\n",
       "      <td>1692</td>\n",
       "      <td>3247</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-20 06:06:44</td>\n",
       "      <td>Same folks said daikon paste could treat a cyt...</td>\n",
       "      <td>['PfizerBioNTech']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1338158543359250433</td>\n",
       "      <td>Albert Fong</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>Marketing dude, tech geek, heavy metal &amp; '80s ...</td>\n",
       "      <td>2009-09-21 15:27:30</td>\n",
       "      <td>834</td>\n",
       "      <td>666</td>\n",
       "      <td>178</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-13 16:27:13</td>\n",
       "      <td>While the world has been on the wrong side of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1337858199140118533</td>\n",
       "      <td>eliüá±üáπüá™üá∫üëå</td>\n",
       "      <td>Your Bed</td>\n",
       "      <td>heil, hydra üñê‚ò∫</td>\n",
       "      <td>2020-06-25 23:30:28</td>\n",
       "      <td>10</td>\n",
       "      <td>88</td>\n",
       "      <td>155</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:33:45</td>\n",
       "      <td>#coronavirus #SputnikV #AstraZeneca #PfizerBio...</td>\n",
       "      <td>['coronavirus', 'SputnikV', 'AstraZeneca', 'Pf...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1337855739918835717</td>\n",
       "      <td>Charles Adler</td>\n",
       "      <td>Vancouver, BC - Canada</td>\n",
       "      <td>Hosting \"CharlesAdlerTonight\" Global News Radi...</td>\n",
       "      <td>2008-09-10 11:28:53</td>\n",
       "      <td>49165</td>\n",
       "      <td>3933</td>\n",
       "      <td>21853</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-12-12 20:23:59</td>\n",
       "      <td>Facts are immutable, Senator, even when you're...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>446</td>\n",
       "      <td>2129</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1337854064604966912</td>\n",
       "      <td>Citizen News Channel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Citizen News Channel bringing you an alternati...</td>\n",
       "      <td>2020-04-23 17:58:42</td>\n",
       "      <td>152</td>\n",
       "      <td>580</td>\n",
       "      <td>1473</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:17:19</td>\n",
       "      <td>Explain to me again why we need a vaccine @Bor...</td>\n",
       "      <td>['whereareallthesickpeople', 'PfizerBioNTech']</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id             user_name              user_location  \\\n",
       "0  1340539111971516416            Rachel Roh  La Crescenta-Montrose, CA   \n",
       "1  1338158543359250433           Albert Fong          San Francisco, CA   \n",
       "2  1337858199140118533              eliüá±üáπüá™üá∫üëå                   Your Bed   \n",
       "3  1337855739918835717         Charles Adler     Vancouver, BC - Canada   \n",
       "4  1337854064604966912  Citizen News Channel                        NaN   \n",
       "\n",
       "                                    user_description         user_created  \\\n",
       "0  Aggregator of Asian American news; scanning di...  2009-04-08 17:52:46   \n",
       "1  Marketing dude, tech geek, heavy metal & '80s ...  2009-09-21 15:27:30   \n",
       "2                                     heil, hydra üñê‚ò∫  2020-06-25 23:30:28   \n",
       "3  Hosting \"CharlesAdlerTonight\" Global News Radi...  2008-09-10 11:28:53   \n",
       "4  Citizen News Channel bringing you an alternati...  2020-04-23 17:58:42   \n",
       "\n",
       "   user_followers  user_friends  user_favourites  user_verified  \\\n",
       "0             405          1692             3247          False   \n",
       "1             834           666              178          False   \n",
       "2              10            88              155          False   \n",
       "3           49165          3933            21853           True   \n",
       "4             152           580             1473          False   \n",
       "\n",
       "                  date                                               text  \\\n",
       "0  2020-12-20 06:06:44  Same folks said daikon paste could treat a cyt...   \n",
       "1  2020-12-13 16:27:13  While the world has been on the wrong side of ...   \n",
       "2  2020-12-12 20:33:45  #coronavirus #SputnikV #AstraZeneca #PfizerBio...   \n",
       "3  2020-12-12 20:23:59  Facts are immutable, Senator, even when you're...   \n",
       "4  2020-12-12 20:17:19  Explain to me again why we need a vaccine @Bor...   \n",
       "\n",
       "                                            hashtags               source  \\\n",
       "0                                 ['PfizerBioNTech']  Twitter for Android   \n",
       "1                                                NaN      Twitter Web App   \n",
       "2  ['coronavirus', 'SputnikV', 'AstraZeneca', 'Pf...  Twitter for Android   \n",
       "3                                                NaN      Twitter Web App   \n",
       "4     ['whereareallthesickpeople', 'PfizerBioNTech']   Twitter for iPhone   \n",
       "\n",
       "   retweets  favorites  is_retweet  \n",
       "0         0          0       False  \n",
       "1         1          1       False  \n",
       "2         0          0       False  \n",
       "3       446       2129       False  \n",
       "4         0          0       False  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use only the first 1000 tweets for an example to reduce processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[0:999,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to create date column of month_year\n",
    "\n",
    "def date_create(created_at):\n",
    "    ''' Getting YYYY_MM from created_at '''\n",
    "    year = created_at.split(\"-\")[0]\n",
    "    month = created_at.split(\"-\")[1]\n",
    "    year_month = year + \"_\" + month\n",
    "    return year_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for removing emojis and special characters from tweets\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#very basic cleaning to clean text from tweets\n",
    "#basic cleaning: \n",
    "#1 lowercase, \n",
    "#2 remove emojis,\n",
    "#3 use regex to remove urls, \n",
    "#4 use regex to remove punctuation,\n",
    "#5 remove user handles, which come after @ sign\n",
    "#6 extra spaces, etc.\n",
    "def clean_tweet(text):\n",
    "    '''Text Preprocessing '''\n",
    "    text = text.lower() #1\n",
    "    text = remove_emojis(text) #2\n",
    "    text = re.sub(r'http\\S+', '', text) #3\n",
    "    text = re.sub(r'[^\\w\\d\\s\\']+', '', text) #4\n",
    "    text = re.sub('@[^\\s]+','',text) #5\n",
    "  \n",
    "    text = re.sub(\"^\\s+|\\s+$\", \"\", text, flags=re.UNICODE) #6\n",
    "    text = \" \".join(re.split(\"\\s+\", text, flags=re.UNICODE)) #6\n",
    "   \n",
    "    return text\n",
    "\n",
    "df.loc[:, \"clean_text\"] = df.loc[:, \"text\"].apply(clean_tweet) #run function for each tweet\n",
    "df.loc[:, \"year_month\"] = df.loc[:, \"user_created\"].apply(date_create)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download stopwords\n",
    "nltk has a list of predefined stop words for the english language.\n",
    "\n",
    "Stop words are the most commonly used words in a language and don't often provide much value in discerning the meaning of the text data, so they are removed to reduce dimensionality. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'via', 'also', 'amp', 'do', 'will', 'did', 'does', 'should', 'are', 'could', 'had', 'has', ' have', 'is', 'might', 'must', 'need', 'shall', 'should', 'was', 'were', 'will', 'would', 'bc', 'yo', 'etc']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tierney/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# use the predefined list of english language stopwords from nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords_eng = stopwords.words('english')\n",
    "\n",
    "# for tweets, we might want to add some to make it customized\n",
    "append_words = [\"via\", \"also\", \"amp\",\n",
    "               \"do\", \"will\", \"did\", \"does\", \"should\",\n",
    "               \"are\", \"could\", \"had\", \"has\",\" have\",\n",
    "               \"is\", \"might\", \"must\", \"need\", \"shall\",\n",
    "               \"should\", \"was\", \"were\", \"will\", \"would\",\n",
    "               \"bc\", \"yo\", \"etc\"] \n",
    "stopwords_eng.extend(append_words)\n",
    "print(stopwords_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "One way to reduce the dimensionality of the text data is to use lemmatization, which aims to change words to their root form. This would change plurals to singulars for nouns, or past tense to present for verbs.\n",
    "\n",
    "nltk has a built-in lemmatization algorithm that has been made from WordNet, and incorporates the word's part of speech, and surrounding words when lemmatizing. It's a bit more sophisticated than the also common but simpler approach of stemming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "wnl = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arch'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize(\"arches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downside is that you have to supply the part of speech for it to correctly lemmatize verbs. See below, just using the lemmatize function, the default part of speech is noun, so the word listening remains unchanged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'listening'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize('listening')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'listen'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl.lemmatize('listening', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, instead of using nltk's lemmatization function we'll try spacy's instead. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text lemmatization using spacy\n",
    "nlp = spacy.load('en_core_web_sm') #lemmatization\n",
    "\n",
    "def lemm(text):\n",
    "    text = nlp(text)\n",
    "    tokens = []\n",
    "    for token in text:\n",
    "        tokens.append(token)\n",
    "    text = \" \".join([token.lemma_ for token in text])\n",
    "   \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'lemm_Text'] = df.loc[:, 'clean_text'].apply(lemm) #run function for each shot-related tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_created</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_friends</th>\n",
       "      <th>user_favourites</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>source</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>year_month</th>\n",
       "      <th>lemm_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1340539111971516416</td>\n",
       "      <td>Rachel Roh</td>\n",
       "      <td>La Crescenta-Montrose, CA</td>\n",
       "      <td>Aggregator of Asian American news; scanning di...</td>\n",
       "      <td>2009-04-08 17:52:46</td>\n",
       "      <td>405</td>\n",
       "      <td>1692</td>\n",
       "      <td>3247</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-20 06:06:44</td>\n",
       "      <td>Same folks said daikon paste could treat a cyt...</td>\n",
       "      <td>['PfizerBioNTech']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>same folks said daikon paste could treat a cyt...</td>\n",
       "      <td>2009_04</td>\n",
       "      <td>same folk say daikon paste could treat a cytok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1338158543359250433</td>\n",
       "      <td>Albert Fong</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>Marketing dude, tech geek, heavy metal &amp; '80s ...</td>\n",
       "      <td>2009-09-21 15:27:30</td>\n",
       "      <td>834</td>\n",
       "      <td>666</td>\n",
       "      <td>178</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-13 16:27:13</td>\n",
       "      <td>While the world has been on the wrong side of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>while the world has been on the wrong side of ...</td>\n",
       "      <td>2009_09</td>\n",
       "      <td>while the world have be on the wrong side of h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1337858199140118533</td>\n",
       "      <td>eliüá±üáπüá™üá∫üëå</td>\n",
       "      <td>Your Bed</td>\n",
       "      <td>heil, hydra üñê‚ò∫</td>\n",
       "      <td>2020-06-25 23:30:28</td>\n",
       "      <td>10</td>\n",
       "      <td>88</td>\n",
       "      <td>155</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:33:45</td>\n",
       "      <td>#coronavirus #SputnikV #AstraZeneca #PfizerBio...</td>\n",
       "      <td>['coronavirus', 'SputnikV', 'AstraZeneca', 'Pf...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>coronavirus sputnikv astrazeneca pfizerbiontec...</td>\n",
       "      <td>2020_06</td>\n",
       "      <td>coronavirus sputnikv astrazeneca pfizerbiontec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1337855739918835717</td>\n",
       "      <td>Charles Adler</td>\n",
       "      <td>Vancouver, BC - Canada</td>\n",
       "      <td>Hosting \"CharlesAdlerTonight\" Global News Radi...</td>\n",
       "      <td>2008-09-10 11:28:53</td>\n",
       "      <td>49165</td>\n",
       "      <td>3933</td>\n",
       "      <td>21853</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-12-12 20:23:59</td>\n",
       "      <td>Facts are immutable, Senator, even when you're...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>446</td>\n",
       "      <td>2129</td>\n",
       "      <td>False</td>\n",
       "      <td>facts are immutable senator even when you're n...</td>\n",
       "      <td>2008_09</td>\n",
       "      <td>fact be immutable senator even when you be not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1337854064604966912</td>\n",
       "      <td>Citizen News Channel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Citizen News Channel bringing you an alternati...</td>\n",
       "      <td>2020-04-23 17:58:42</td>\n",
       "      <td>152</td>\n",
       "      <td>580</td>\n",
       "      <td>1473</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:17:19</td>\n",
       "      <td>Explain to me again why we need a vaccine @Bor...</td>\n",
       "      <td>['whereareallthesickpeople', 'PfizerBioNTech']</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>explain to me again why we need a vaccine bori...</td>\n",
       "      <td>2020_04</td>\n",
       "      <td>explain to I again why we need a vaccine boris...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id             user_name              user_location  \\\n",
       "0  1340539111971516416            Rachel Roh  La Crescenta-Montrose, CA   \n",
       "1  1338158543359250433           Albert Fong          San Francisco, CA   \n",
       "2  1337858199140118533              eliüá±üáπüá™üá∫üëå                   Your Bed   \n",
       "3  1337855739918835717         Charles Adler     Vancouver, BC - Canada   \n",
       "4  1337854064604966912  Citizen News Channel                        NaN   \n",
       "\n",
       "                                    user_description         user_created  \\\n",
       "0  Aggregator of Asian American news; scanning di...  2009-04-08 17:52:46   \n",
       "1  Marketing dude, tech geek, heavy metal & '80s ...  2009-09-21 15:27:30   \n",
       "2                                     heil, hydra üñê‚ò∫  2020-06-25 23:30:28   \n",
       "3  Hosting \"CharlesAdlerTonight\" Global News Radi...  2008-09-10 11:28:53   \n",
       "4  Citizen News Channel bringing you an alternati...  2020-04-23 17:58:42   \n",
       "\n",
       "   user_followers  user_friends  user_favourites  user_verified  \\\n",
       "0             405          1692             3247          False   \n",
       "1             834           666              178          False   \n",
       "2              10            88              155          False   \n",
       "3           49165          3933            21853           True   \n",
       "4             152           580             1473          False   \n",
       "\n",
       "                  date                                               text  \\\n",
       "0  2020-12-20 06:06:44  Same folks said daikon paste could treat a cyt...   \n",
       "1  2020-12-13 16:27:13  While the world has been on the wrong side of ...   \n",
       "2  2020-12-12 20:33:45  #coronavirus #SputnikV #AstraZeneca #PfizerBio...   \n",
       "3  2020-12-12 20:23:59  Facts are immutable, Senator, even when you're...   \n",
       "4  2020-12-12 20:17:19  Explain to me again why we need a vaccine @Bor...   \n",
       "\n",
       "                                            hashtags               source  \\\n",
       "0                                 ['PfizerBioNTech']  Twitter for Android   \n",
       "1                                                NaN      Twitter Web App   \n",
       "2  ['coronavirus', 'SputnikV', 'AstraZeneca', 'Pf...  Twitter for Android   \n",
       "3                                                NaN      Twitter Web App   \n",
       "4     ['whereareallthesickpeople', 'PfizerBioNTech']   Twitter for iPhone   \n",
       "\n",
       "   retweets  favorites  is_retweet  \\\n",
       "0         0          0       False   \n",
       "1         1          1       False   \n",
       "2         0          0       False   \n",
       "3       446       2129       False   \n",
       "4         0          0       False   \n",
       "\n",
       "                                          clean_text year_month  \\\n",
       "0  same folks said daikon paste could treat a cyt...    2009_04   \n",
       "1  while the world has been on the wrong side of ...    2009_09   \n",
       "2  coronavirus sputnikv astrazeneca pfizerbiontec...    2020_06   \n",
       "3  facts are immutable senator even when you're n...    2008_09   \n",
       "4  explain to me again why we need a vaccine bori...    2020_04   \n",
       "\n",
       "                                           lemm_Text  \n",
       "0  same folk say daikon paste could treat a cytok...  \n",
       "1  while the world have be on the wrong side of h...  \n",
       "2  coronavirus sputnikv astrazeneca pfizerbiontec...  \n",
       "3  fact be immutable senator even when you be not...  \n",
       "4  explain to I again why we need a vaccine boris...  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords from tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess by removing stop words manually\n",
    "# using list comprehension\n",
    "# output is a list of words\n",
    "def preprocess(tweet):\n",
    "    return [w for w in tweet.lower().split() if w not in stopwords_eng]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_created</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_friends</th>\n",
       "      <th>user_favourites</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>source</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>year_month</th>\n",
       "      <th>lemm_Text</th>\n",
       "      <th>clean_text_nostops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1340539111971516416</td>\n",
       "      <td>Rachel Roh</td>\n",
       "      <td>La Crescenta-Montrose, CA</td>\n",
       "      <td>Aggregator of Asian American news; scanning di...</td>\n",
       "      <td>2009-04-08 17:52:46</td>\n",
       "      <td>405</td>\n",
       "      <td>1692</td>\n",
       "      <td>3247</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-20 06:06:44</td>\n",
       "      <td>Same folks said daikon paste could treat a cyt...</td>\n",
       "      <td>['PfizerBioNTech']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>same folks said daikon paste could treat a cyt...</td>\n",
       "      <td>2009_04</td>\n",
       "      <td>same folk say daikon paste could treat a cytok...</td>\n",
       "      <td>[folk, say, daikon, paste, treat, cytokine, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1338158543359250433</td>\n",
       "      <td>Albert Fong</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>Marketing dude, tech geek, heavy metal &amp; '80s ...</td>\n",
       "      <td>2009-09-21 15:27:30</td>\n",
       "      <td>834</td>\n",
       "      <td>666</td>\n",
       "      <td>178</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-13 16:27:13</td>\n",
       "      <td>While the world has been on the wrong side of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>while the world has been on the wrong side of ...</td>\n",
       "      <td>2009_09</td>\n",
       "      <td>while the world have be on the wrong side of h...</td>\n",
       "      <td>[world, wrong, side, history, year, hopefully,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1337858199140118533</td>\n",
       "      <td>eliüá±üáπüá™üá∫üëå</td>\n",
       "      <td>Your Bed</td>\n",
       "      <td>heil, hydra üñê‚ò∫</td>\n",
       "      <td>2020-06-25 23:30:28</td>\n",
       "      <td>10</td>\n",
       "      <td>88</td>\n",
       "      <td>155</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:33:45</td>\n",
       "      <td>#coronavirus #SputnikV #AstraZeneca #PfizerBio...</td>\n",
       "      <td>['coronavirus', 'SputnikV', 'AstraZeneca', 'Pf...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>coronavirus sputnikv astrazeneca pfizerbiontec...</td>\n",
       "      <td>2020_06</td>\n",
       "      <td>coronavirus sputnikv astrazeneca pfizerbiontec...</td>\n",
       "      <td>[coronavirus, sputnikv, astrazeneca, pfizerbio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1337855739918835717</td>\n",
       "      <td>Charles Adler</td>\n",
       "      <td>Vancouver, BC - Canada</td>\n",
       "      <td>Hosting \"CharlesAdlerTonight\" Global News Radi...</td>\n",
       "      <td>2008-09-10 11:28:53</td>\n",
       "      <td>49165</td>\n",
       "      <td>3933</td>\n",
       "      <td>21853</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-12-12 20:23:59</td>\n",
       "      <td>Facts are immutable, Senator, even when you're...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>446</td>\n",
       "      <td>2129</td>\n",
       "      <td>False</td>\n",
       "      <td>facts are immutable senator even when you're n...</td>\n",
       "      <td>2008_09</td>\n",
       "      <td>fact be immutable senator even when you be not...</td>\n",
       "      <td>[fact, immutable, senator, even, ethically, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1337854064604966912</td>\n",
       "      <td>Citizen News Channel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Citizen News Channel bringing you an alternati...</td>\n",
       "      <td>2020-04-23 17:58:42</td>\n",
       "      <td>152</td>\n",
       "      <td>580</td>\n",
       "      <td>1473</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:17:19</td>\n",
       "      <td>Explain to me again why we need a vaccine @Bor...</td>\n",
       "      <td>['whereareallthesickpeople', 'PfizerBioNTech']</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>explain to me again why we need a vaccine bori...</td>\n",
       "      <td>2020_04</td>\n",
       "      <td>explain to I again why we need a vaccine boris...</td>\n",
       "      <td>[explain, vaccine, borisjohnson, matthancock, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id             user_name              user_location  \\\n",
       "0  1340539111971516416            Rachel Roh  La Crescenta-Montrose, CA   \n",
       "1  1338158543359250433           Albert Fong          San Francisco, CA   \n",
       "2  1337858199140118533              eliüá±üáπüá™üá∫üëå                   Your Bed   \n",
       "3  1337855739918835717         Charles Adler     Vancouver, BC - Canada   \n",
       "4  1337854064604966912  Citizen News Channel                        NaN   \n",
       "\n",
       "                                    user_description         user_created  \\\n",
       "0  Aggregator of Asian American news; scanning di...  2009-04-08 17:52:46   \n",
       "1  Marketing dude, tech geek, heavy metal & '80s ...  2009-09-21 15:27:30   \n",
       "2                                     heil, hydra üñê‚ò∫  2020-06-25 23:30:28   \n",
       "3  Hosting \"CharlesAdlerTonight\" Global News Radi...  2008-09-10 11:28:53   \n",
       "4  Citizen News Channel bringing you an alternati...  2020-04-23 17:58:42   \n",
       "\n",
       "   user_followers  user_friends  user_favourites  user_verified  \\\n",
       "0             405          1692             3247          False   \n",
       "1             834           666              178          False   \n",
       "2              10            88              155          False   \n",
       "3           49165          3933            21853           True   \n",
       "4             152           580             1473          False   \n",
       "\n",
       "                  date                                               text  \\\n",
       "0  2020-12-20 06:06:44  Same folks said daikon paste could treat a cyt...   \n",
       "1  2020-12-13 16:27:13  While the world has been on the wrong side of ...   \n",
       "2  2020-12-12 20:33:45  #coronavirus #SputnikV #AstraZeneca #PfizerBio...   \n",
       "3  2020-12-12 20:23:59  Facts are immutable, Senator, even when you're...   \n",
       "4  2020-12-12 20:17:19  Explain to me again why we need a vaccine @Bor...   \n",
       "\n",
       "                                            hashtags               source  \\\n",
       "0                                 ['PfizerBioNTech']  Twitter for Android   \n",
       "1                                                NaN      Twitter Web App   \n",
       "2  ['coronavirus', 'SputnikV', 'AstraZeneca', 'Pf...  Twitter for Android   \n",
       "3                                                NaN      Twitter Web App   \n",
       "4     ['whereareallthesickpeople', 'PfizerBioNTech']   Twitter for iPhone   \n",
       "\n",
       "   retweets  favorites  is_retweet  \\\n",
       "0         0          0       False   \n",
       "1         1          1       False   \n",
       "2         0          0       False   \n",
       "3       446       2129       False   \n",
       "4         0          0       False   \n",
       "\n",
       "                                          clean_text year_month  \\\n",
       "0  same folks said daikon paste could treat a cyt...    2009_04   \n",
       "1  while the world has been on the wrong side of ...    2009_09   \n",
       "2  coronavirus sputnikv astrazeneca pfizerbiontec...    2020_06   \n",
       "3  facts are immutable senator even when you're n...    2008_09   \n",
       "4  explain to me again why we need a vaccine bori...    2020_04   \n",
       "\n",
       "                                           lemm_Text  \\\n",
       "0  same folk say daikon paste could treat a cytok...   \n",
       "1  while the world have be on the wrong side of h...   \n",
       "2  coronavirus sputnikv astrazeneca pfizerbiontec...   \n",
       "3  fact be immutable senator even when you be not...   \n",
       "4  explain to I again why we need a vaccine boris...   \n",
       "\n",
       "                                  clean_text_nostops  \n",
       "0  [folk, say, daikon, paste, treat, cytokine, st...  \n",
       "1  [world, wrong, side, history, year, hopefully,...  \n",
       "2  [coronavirus, sputnikv, astrazeneca, pfizerbio...  \n",
       "3  [fact, immutable, senator, even, ethically, st...  \n",
       "4  [explain, vaccine, borisjohnson, matthancock, ...  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text_nostops'] = df['lemm_Text'].apply(preprocess)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize text and create a tfidf matrix\n",
    "\n",
    "tfidf stands for term frequency inverse document frequency\n",
    "\n",
    "It is a common way to measure the importance of a word in a document by indicating how common it is in that document, compared to all other documents in the data.\n",
    "\n",
    "Here documents are tweets. So term frequency refers to how many times a word is used in a given tweet, and document frequency is the number of times a word is used in any tweet. Thus if a word gets used a lot in a single tweet but is rare, it's going to have a high value in the tf-idf matrix and is assumed to have a lot of meaning in the tweet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because nltk doesn't have a tfidf function, let's use sci kit learn's built in one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a matrix with one row for each tweet and one row for each token or word in any of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2911)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(min_df = 0.00017, stop_words=stopwords_eng, ngram_range = (1,1)).fit(df.lemm_Text)\n",
    "x = vect.fit_transform(df.lemm_Text)\n",
    "x_df = pd.DataFrame(x.toarray(), columns = vect.get_feature_names_out())\n",
    "x_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity\n",
    "An example of a usecase for the tfidf matrix is for us to examine tweets that are similar to one another based on the words that are in them. Cosine similarity is a useful similarity or distance based metric for text data, as it doesn't matter how long the documents are. \n",
    "\n",
    "Tweets that are exactly alike will score 1, and ones that don't have any matching words will score 0. \n",
    "\n",
    "Let's see if we can find similar tweets to one another using this metric out of the first 1000 tweets about COVID-19 vaccines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019472</td>\n",
       "      <td>0.056724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009712</td>\n",
       "      <td>0.009064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017503</td>\n",
       "      <td>0.050329</td>\n",
       "      <td>0.009050</td>\n",
       "      <td>0.008457</td>\n",
       "      <td>0.008199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.009378</td>\n",
       "      <td>0.079026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025060</td>\n",
       "      <td>0.009571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021111</td>\n",
       "      <td>0.019703</td>\n",
       "      <td>0.089136</td>\n",
       "      <td>0.021602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018622</td>\n",
       "      <td>0.008913</td>\n",
       "      <td>0.019671</td>\n",
       "      <td>0.008997</td>\n",
       "      <td>0.017822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025954</td>\n",
       "      <td>0.024223</td>\n",
       "      <td>0.025935</td>\n",
       "      <td>0.026558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022894</td>\n",
       "      <td>0.010957</td>\n",
       "      <td>0.024184</td>\n",
       "      <td>0.011061</td>\n",
       "      <td>0.021911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.017503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018622</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158297</td>\n",
       "      <td>0.119215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059462</td>\n",
       "      <td>0.055495</td>\n",
       "      <td>0.039316</td>\n",
       "      <td>0.019735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017971</td>\n",
       "      <td>0.051775</td>\n",
       "      <td>0.050198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.050329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010957</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018507</td>\n",
       "      <td>0.051686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009231</td>\n",
       "      <td>0.008615</td>\n",
       "      <td>0.054492</td>\n",
       "      <td>0.009446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.009050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024184</td>\n",
       "      <td>0.009236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020373</td>\n",
       "      <td>0.019014</td>\n",
       "      <td>0.020358</td>\n",
       "      <td>0.020847</td>\n",
       "      <td>0.106127</td>\n",
       "      <td>0.017971</td>\n",
       "      <td>0.089847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008683</td>\n",
       "      <td>0.017199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.008457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028729</td>\n",
       "      <td>0.026813</td>\n",
       "      <td>0.018996</td>\n",
       "      <td>0.009535</td>\n",
       "      <td>0.035192</td>\n",
       "      <td>0.051775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008683</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.008199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021911</td>\n",
       "      <td>0.008368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037278</td>\n",
       "      <td>0.034791</td>\n",
       "      <td>0.036862</td>\n",
       "      <td>0.018888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050198</td>\n",
       "      <td>0.007793</td>\n",
       "      <td>0.017199</td>\n",
       "      <td>0.024253</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2    3         4         5         6    \\\n",
       "0    0.000000  0.000000  0.009378  0.0  0.011529  0.000000  0.000000   \n",
       "1    0.000000  1.000000  0.079026  0.0  0.000000  0.000000  0.037222   \n",
       "2    0.009378  0.079026  1.000000  0.0  0.025060  0.009571  0.000000   \n",
       "3    0.000000  0.000000  0.000000  1.0  0.000000  0.000000  0.000000   \n",
       "4    0.011529  0.000000  0.025060  0.0  0.000000  0.011766  0.000000   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995  0.017503  0.000000  0.018622  0.0  0.022894  0.000000  0.000000   \n",
       "996  0.050329  0.000000  0.008913  0.0  0.010957  0.008197  0.000000   \n",
       "997  0.009050  0.000000  0.019671  0.0  0.024184  0.009236  0.000000   \n",
       "998  0.008457  0.000000  0.008997  0.0  0.011061  0.000000  0.000000   \n",
       "999  0.008199  0.000000  0.017822  0.0  0.021911  0.008368  0.000000   \n",
       "\n",
       "          7         8         9    ...       990       991       992  \\\n",
       "0    0.000000  0.019472  0.056724  ...  0.009712  0.009064  0.000000   \n",
       "1    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2    0.000000  0.181720  0.000000  ...  0.021111  0.019703  0.089136   \n",
       "3    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "4    0.000000  0.052036  0.000000  ...  0.025954  0.024223  0.025935   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995  0.158297  0.119215  0.000000  ...  0.059462  0.055495  0.039316   \n",
       "996  0.000000  0.018507  0.051686  ...  0.009231  0.008615  0.054492   \n",
       "997  0.000000  0.040846  0.000000  ...  0.020373  0.019014  0.020358   \n",
       "998  0.000000  0.057599  0.000000  ...  0.028729  0.026813  0.018996   \n",
       "999  0.000000  0.074738  0.000000  ...  0.037278  0.034791  0.036862   \n",
       "\n",
       "          993       994       995       996       997       998       999  \n",
       "0    0.009938  0.000000  0.017503  0.050329  0.009050  0.008457  0.008199  \n",
       "1    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2    0.021602  0.000000  0.018622  0.008913  0.019671  0.008997  0.017822  \n",
       "3    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4    0.026558  0.000000  0.022894  0.010957  0.024184  0.011061  0.021911  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "995  0.019735  0.000000  0.000000  0.000000  0.017971  0.051775  0.050198  \n",
       "996  0.009446  0.000000  0.000000  0.000000  0.089847  0.000000  0.007793  \n",
       "997  0.020847  0.106127  0.017971  0.089847  0.000000  0.008683  0.017199  \n",
       "998  0.009535  0.035192  0.051775  0.000000  0.008683  1.000000  0.024253  \n",
       "999  0.018888  0.000000  0.050198  0.007793  0.017199  0.024253  1.000000  \n",
       "\n",
       "[1000 rows x 1000 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "cosine_mat = sklearn.metrics.pairwise.cosine_similarity(x_df)\n",
    "# replace diagonal with zero instead of 1 for perfect self-matches\n",
    "cosine_mat[cosine_mat==1]=0\n",
    "cosine_df = pd.DataFrame(cosine_mat)\n",
    "cosine_df.index = x_df.index\n",
    "cosine_df.columns = x_df.index\n",
    "cosine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100         NaN\n",
       "101    0.375933\n",
       "102         NaN\n",
       "103         NaN\n",
       "104         NaN\n",
       "105         NaN\n",
       "106    0.316697\n",
       "107    0.247538\n",
       "108         NaN\n",
       "109    0.836042\n",
       "110    0.296032\n",
       "111         NaN\n",
       "112         NaN\n",
       "113    0.176355\n",
       "114         NaN\n",
       "115    0.232486\n",
       "116    0.381110\n",
       "117    1.000000\n",
       "118         NaN\n",
       "119    0.307711\n",
       "120    0.119351\n",
       "dtype: float64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_df.max().where(cosine_df.max()<1).loc[100:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find two tweets that are similar:\n",
    "Here we compare tweet number 109 to it's highest cosine similarity match: tweet number 108 with a score of 0.836."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8360415108175331"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_df.loc[109,].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_df.loc[109,].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#BreakingNews A nurse in New York City on Monday became the first person in the United States to receive the corona‚Ä¶ https://t.co/02Mu5HKYs5'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[108,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#UPDATE A nurse in New York City on Monday became the first person in the United States to receive the coronavirus‚Ä¶ https://t.co/N4j8xorUzO'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[109,'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very close match!\n",
    "\n",
    "This could be a useful tool to use for a clustering algorithm, or to use for stratified sampling of tweets for annotation, to ensure that your annotators are given a diverse sample of tweets to label. \n",
    "\n",
    "An alternative to cosine similarity is \"soft cosine similarity\" and is another option that takes into account words meanings, rather than looking for exact matches. This can be done using gensim's library but that's for another notebook. \n",
    "\n",
    "\n",
    "Overall, I think that nltk and spaCy were both helpful to get the preprocessing of the tweet text data finished. It seems like SpaCy has better documentation, especially with their free web course for using it for NLP with code and everything. However, for researchers it may be problematic since the algorithms are chosen for you to optimize processing speeds and may limit scientific reproducibility, documentation, and the users ability to adjust presets. It seems like spacy is more popular for developers and nltk more popular with researchers, especially linguistics. Overall, it's good to be able to integrate tools from both in a text pre-processing pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmi6018",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
